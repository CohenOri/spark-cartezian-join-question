{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Question\n",
    "Assume I have Dataframe #1 (csv like) of this type:\n",
    "Dataframe #1:\n",
    "```\n",
    "id1,id2\n",
    "id2,id3\n",
    "idX,idY, idZ\n",
    "etc.\n",
    "```\n",
    "(length of each row is different, up to M \"columns\")\n",
    "\n",
    "And another Dataframe #2:\n",
    "```id1,id2 -> data1\n",
    "id3,id4 -> data2\n",
    "id5, id6 -> data3\n",
    "id2 -> data4\n",
    "id1,id2,id3,id4,id100 -> data1700\n",
    "etc.\n",
    "```\n",
    "(length of each row is different, I have more rows than in Dataframe #1, and they generally longer, up to N columns)\n",
    "\n",
    "We want to get for each row of \"Dataframe #1\" the dataX **only if all the ids from Dataframe #1 are fully contained in Dataframe #2 row**.\n",
    "\n",
    "Example:\n",
    "First row of Dataframe #1, (id1,id2) are fully contained in first row of Dataframe #2, (id1,id2 -> data1) therefore we **return** data1,\n",
    "it also fully contained in (id1,id2,id3,id4,id100 -> data1700) so we **return** data1700. It isn't fully contained in (id2 -> data4) therefore we will **NOT return** data4\n",
    "\n",
    "How would you do such a thing using spark?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install Spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-17T13:18:12.601193Z",
     "end_time": "2023-05-17T13:18:14.836743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.3.2 in ./venv/lib/python3.8/site-packages (3.3.2)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in ./venv/lib/python3.8/site-packages (from pyspark==3.3.2) (0.10.9.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a Spark Session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x108cbba60>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://172.16.27.142:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>InterviewQuestion</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"InterviewQuestion\").getOrCreate() # Check Spark Session Information\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T13:18:14.844635Z",
     "end_time": "2023-05-17T13:18:14.903877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1:\n",
      "+------+------+\n",
      "|T1_id1|T1_id2|\n",
      "+------+------+\n",
      "|     1|     2|\n",
      "|     3|     4|\n",
      "|     5|     6|\n",
      "|  null|     9|\n",
      "+------+------+\n",
      "\n",
      "df2:\n",
      "+------+------+------+-----+\n",
      "|T2_id1|T2_id2|T2_id3| data|\n",
      "+------+------+------+-----+\n",
      "|     1|     2|     3|data1|\n",
      "|     1|     2|  null|data2|\n",
      "|     1|     3|  null|data3|\n",
      "|     3|  null|  null|data4|\n",
      "|     4|  null|  null|data5|\n",
      "|     5|     6|  null|dataS|\n",
      "|     5|     9|  null|dataX|\n",
      "|  null|     9|  null|data9|\n",
      "+------+------+------+-----+\n",
      "\n",
      "cartesian_df:\n",
      "+------+------+------+------+------+-----+\n",
      "|T1_id1|T1_id2|T2_id1|T2_id2|T2_id3| data|\n",
      "+------+------+------+------+------+-----+\n",
      "|     1|     2|     1|     2|     3|data1|\n",
      "|     1|     2|     1|     2|  null|data2|\n",
      "|     1|     2|     1|     3|  null|data3|\n",
      "|     1|     2|     3|  null|  null|data4|\n",
      "|     1|     2|     4|  null|  null|data5|\n",
      "|     1|     2|     5|     6|  null|dataS|\n",
      "|     1|     2|     5|     9|  null|dataX|\n",
      "|     1|     2|  null|     9|  null|data9|\n",
      "|     3|     4|     1|     2|     3|data1|\n",
      "|     3|     4|     1|     2|  null|data2|\n",
      "|     3|     4|     1|     3|  null|data3|\n",
      "|     3|     4|     3|  null|  null|data4|\n",
      "|     3|     4|     4|  null|  null|data5|\n",
      "|     3|     4|     5|     6|  null|dataS|\n",
      "|     3|     4|     5|     9|  null|dataX|\n",
      "|     3|     4|  null|     9|  null|data9|\n",
      "|     5|     6|     1|     2|     3|data1|\n",
      "|     5|     6|     1|     2|  null|data2|\n",
      "|     5|     6|     1|     3|  null|data3|\n",
      "|     5|     6|     3|  null|  null|data4|\n",
      "|     5|     6|     4|  null|  null|data5|\n",
      "|     5|     6|     5|     6|  null|dataS|\n",
      "|     5|     6|     5|     9|  null|dataX|\n",
      "|     5|     6|  null|     9|  null|data9|\n",
      "|  null|     9|     1|     2|     3|data1|\n",
      "|  null|     9|     1|     2|  null|data2|\n",
      "|  null|     9|     1|     3|  null|data3|\n",
      "|  null|     9|     3|  null|  null|data4|\n",
      "|  null|     9|     4|  null|  null|data5|\n",
      "|  null|     9|     5|     6|  null|dataS|\n",
      "|  null|     9|     5|     9|  null|dataX|\n",
      "|  null|     9|  null|     9|  null|data9|\n",
      "+------+------+------+------+------+-----+\n",
      "\n",
      "filtered_df:\n",
      "+------+------+------+------+------+-----+\n",
      "|T1_id1|T1_id2|T2_id1|T2_id2|T2_id3| data|\n",
      "+------+------+------+------+------+-----+\n",
      "|     1|     2|     1|     2|     3|data1|\n",
      "|     1|     2|     1|     2|  null|data2|\n",
      "|     5|     6|     5|     6|  null|dataS|\n",
      "|  null|     9|     5|     9|  null|dataX|\n",
      "|  null|     9|  null|     9|  null|data9|\n",
      "+------+------+------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql.functions import col, array\n",
    "\n",
    "# create example dataframes\n",
    "df1 = spark.createDataFrame([(1, 2), (3, 4), (5, 6), (None, 9)], [\"T1_id1\", \"T1_id2\"])\n",
    "df2 = spark.createDataFrame([(1, 2, 3, \"data1\"), (1, 2, None, \"data2\") ,(1, 3, None, \"data3\"), (3, None, None, \"data4\"), (4, None, None, \"data5\"), (5,6, None, \"dataS\"), (5,9, None, \"dataX\"), (None,9, None, \"data9\")], [\"T2_id1\", \"T2_id2\", \"T2_id3\", \"data\"])\n",
    "# Note I have an implicit assumption that each row in df1/df2 is of the same size, and it's not necessary the case, one workaround could be padding nulls in order to make sure each row have the same amount of columns.\n",
    "\n",
    "# perform cartesian join between df1 and df2\n",
    "cartesian_df = df1.crossJoin(df2)\n",
    "\n",
    "# filter the cartesian join to keep only the rows where the key from df1 is fully contained in the key from df2, if the key is None its considered \"contained\":\n",
    "filtered_df = cartesian_df.filter(\n",
    "    (col(\"T1_id1\").isNull() | array_contains(array(col(\"T2_id1\"), col(\"T2_id2\"), col(\"T2_id3\")), col(\"T1_id1\"))) &\n",
    "    (col(\"T1_id2\").isNull() | array_contains(array(col(\"T2_id1\"), col(\"T2_id2\"), col(\"T2_id3\")), col(\"T1_id2\"))))\n",
    "\n",
    "# print the output after every step - for debugging purposes, *NOT for production*\n",
    "print(\"df1:\")\n",
    "df1.show(100)\n",
    "print(\"df2:\")\n",
    "df2.show(100)\n",
    "print(\"cartesian_df:\")\n",
    "cartesian_df.show(100)\n",
    "print(\"filtered_df:\")\n",
    "filtered_df.show(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T13:18:32.955292Z",
     "end_time": "2023-05-17T13:18:34.374182Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
